---
layout: blog
comments: true
title: 【课程】统计学习
course_name: Statistical Learning
cover: SL.jpg
introduction: 本课程参照MIT的9.520:Statistical Learning Theory and Applications整理而成，它从正则(Regularization )的角度全面审视了包括Logistic 回归, 支持向量机，稀疏表示、流形正则等方法在内的多种机器学习方法。
---

>摘要：本课程参照MIT的[9.520:Statistical Learning Theory and Applications](http://www.mit.edu/~9.520/fall14/)整理而成，它从正则(Regularization )的角度全面审视了包括`Logistic 回归`, `支持向量机`，`稀疏表示`、`流形正则`等方法在内的多种机器学习方法。

##介绍

本课程主要讲解统计学习理论框架下的机器学习方法的一些最新进展。这既包括一些经典的方法，如正则网络和支持向量机，也会涉及一些几何(geometry)方法，稀疏(sparsity)，在线学习，特征选择(feature selection)，结构化输出和多任务学习等在内的一些前沿的技术。本课程也会谈到RBF和深度学习网络的联系。

过去15年中，如何有效的`学习`已经成为了研究AI的一个中心问题。

数学理论，算法实现和神经科学是机器学习的三大支柱。


<div align='center'><img src="../img/learnin_math_algorithm_neuroscience.png"  /><p class = "figure_caption">Figure 1.1.数学为机器学习的算法研究提供了理论工具以回答一些基础问题，例如什么样的学习模型具有泛化能力？如何提高学习算法的稳定性？这些问题都需要数学的介入。工程实践是对理论算法的实现，它需要针对实际问题对理论算法进行某种改造。</p></div>

##学习问题和正则

##数学补充(泛函分析和概率论)

##再生核空间

##字典，特征映射Mercer 理论

##Tikhonov 正则和表示理论

##正则最小二乘

##Logistic 回归和支持向量机

##Iterative Regularization via Early Stopping

##稀疏表示

##Proximal Methods

##Multiple Kernel Learning

##Generalization Bounds, Intro to Stability

##Stability of Tikhonov Regularization

##Consistency, Learnability and Regularization    

##Manifold Regularization

##Regularization for Multi-Output Learning

##学习数据表示:深度学习