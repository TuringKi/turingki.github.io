---
layout: blog
comments: true
title: 【笔记】什么是主成分分析？（Updated:2015-5-17）
---
原文：[What is principal component analysis](../img/What is principal component analysis.pdf)

###摘要

|PCA(Principal Component Analysis)，主成分分析，是一种广泛使用的降维技术。它通过将属于某个高维空间$H$的数据集，<b>在尽量保留数据集主要信息的情况下</b>，线性变换到$H$的某个低维子空间$h$，以方便对数据集进行后续处理。|

###1.均值：0维PCA

>最简单的数据降维方法，当然是用数据集所在空间的一个点来描述数据集，即所谓的<b>0维PCA</b>。

给定一组样本$S = \{x_1,x_2,\dots,x_n\} \in R^N$,我们要找到这样的一个$m \in R^N$,使得$m$能够<b>尽量准确地</b>表征$S$整体信息。那么可以构造：

$$
E=\sum_{i = 1}^{n}\|m - x_i\|_2^2
$$

即使得$m$与$S$中的每个样本的欧氏距离最小。对上式我们可以很容易的求解出$m = \frac{1}{n}\sum_{i = 1}^{n}x_i$，这就是$S$的样本<b>均值</b>。这也与我们的直观认识是符合的：用一组数据的平均值粗略地描述这组数据的整体性质。比如说用平均年龄衡量一个公司的员工的“年轻”程度；用平均身高评价一个篮球队的是否具有身体优势。

然而这样的表述数据性质的方式还是太过于粗略了，均值某种程度上只能反映出数据集的全局(Global)性质。而更多的情况下，我们需要知道数据的一些更加“局部”(local)的信息，例如数据集中的样本点在空间中的分布情况，是密集呢，还是稀疏。举个更加具体的例子：当我们描述一个高斯分布的时候，通常需要两个参数,期望（可以用均值估计）和方差。只给定期望是无法描述一个高斯分布的。按照这样的思路，我们将所谓<b>0维PCA</b>推高一维，尝试用<b>1维PCA</b>描述数据集$S$。

###2.1维PCA

在此之前我们先给出PCA的一般定义:

>`定义1`：给定$n$个样本点$x_1,x_2,\dots,x_n \in R^p$,主成分分析(PCA)寻找这样一个子空间使得它满足:1)维度$k \< p $；2)这n个样本点与各自在此空间的正交映射的欧式距离之和最小。

注意，在研究对象是离散样本点的情况下，数学上我们可以用一个矩阵来近似表征样本所处的子空间。因此我们令$X = [x_1,x_2,\dots,x_n] \in R^{p \times n}$表征原样本空间。那么上述定义可以由下面的目标函数表示：

$$

\hat{X} = argmin_{M:rank(M) = k} \| X - M \|^2_F

$$